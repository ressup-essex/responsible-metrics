[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Responsible Research Metrics Self-Study Module",
    "section": "",
    "text": "Welcome to this self-study module\nThis self-study module covers the core principles of responsible research metrics and be completed in your own time\nThis module is designed for:\n\nAnyone who is interested in how research is measured and assessed\nAnyone involved in recruitment, reward and recognition processes or grant applications where research may be used as an assessment tool\n\nEveryone should complete Section 1, 2, and 3. Section 4 is for those who is involved in recruitment, reward and recognition processes, grant applications as an assessor of people.\nFor help please contact the Library metrics team via eprints@soton.ac.uk\n\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is version 1 of the Responsible Research Metrics Self Study module. The module was written by the Bibliometrics Team at the University of Southampton Library in 2024 and released in xxx 2025.\nThe module team is;\n\nKate Lapage\nClare Hemmings\nLorrayne Smith\n\nThe module’s foundations are taken from the university’s Responsible Research Policy. For help, feedback or suggestions please contact the team via eprints@soton.ac.uk.\nLinks provided to sites external to the University of Southampton were accurate at the time of release.\nThis module is shared under a CC BY license so that it may be adapted to suit other institutions’ policies. We ask that this page remains with further information by the adapting organisation added beneath.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "Section 1.html",
    "href": "Section 1.html",
    "title": "Section 1",
    "section": "",
    "text": "In this first section you will explore:\n\nWhat metrics are in the context of research\nThe principles of responsible metrics\nGuidelines around the responsible use of metrics\n\n\n\nMetrics are a quantitative snapshot of how research outputs perform. A research output is anything that disseminates research; for instance a journal article, a book, an exhibition or software. Metrics can look at one research output, at the outputs from a single researcher or at the entire output of a department or University. The majority of metrics primarily focus on journal articles but metrics may be available for other research types.\nThere are many different types of metrics and they each have a prescribed purpose.\nWhen using metrics it is important to have a question you are trying to answer so you can decide which metrics to use.\nUsing the wrong metric can have real world implications, from making false claims to causing reputational harm, which could impact future career ambitions or cause institutional damage.\n\n\n\nAs research disciplines and outputs are so varied there is no set step-by-step guide to carrying out a metrics analysis. Instead, there are a set of principles that should be followed: Transparency, Appropriateness, Equality, Reproducible and Continual Reassessment.\n\n\nTransparency in metrics means having an explanation in clear, simple language to ensure end-users understand the data used, its reliability and its limitations. \nEssentially what, where and when; \n\nWhat metrics were used? \nWhere did the data come from (sources)? \nWhen was the assessment made? – the assessment is only a snapshot in time as the data sources update over time \n\n\n\n\nThe use of Metrics should be fair.  You should only compare like-for-like i.e. make comparisons within a single discipline, a limited time period, or using a normalised metric.\nConsistency is important. If you can’t apply the same method across your analysis you should not use that method. \nBe aware that individuals may not be directly comparable due to length of service or time out of work such as for maternity leave.\n\nE.g. you should not compare an early career researcher with a researcher who has 20 years of experience unless you have restricted the comparison to a suitable time frame. \n\n\n\n\nAll metrics should be tailored to the focus of the analysis. Use metrics for their intended purpose only.\n\nFor example, you must not use a journal metric to infer the quality of an individual output or a person’s contribution to research.\n\nMetrics should only be used when necessary and should be used in conjunction with expert testimony rather than in isolation. \nWhen assessing a person, metrics must not be used as the sole source of information. This is especially true for employment status, but also for personal reputation in a formal or informal context. \n\nFor example, a highly cited paper might be highly cited because everyone disagrees with it\n\nIt is recommended that you use more than one metric to verify results. \n\n\n\nAnyone should be able to reproduce your results by using the explanation you have provided.\n\n\n\nContinually assess commonly used metrics, especially concerning appropriateness and equality. If a metric is no longer fit for purpose, it should not be used. \n\n\n\n\nA number of guidelines have been published which look to shape our approach to responsible metrics.  The University has its own responsible metrics policy which is based on the principles of DORA. Below are summaries of three key international initiatives, each of which advise on a different aspect: \nDORA (the Declaration of Research Assessment) \nDORA is a set of principles that were published in 2012.  They are designed to ensure that the quality and impact of scientific outputs is “measured accurately and evaluated wisely.”\nDORA focuses particularly on the use of journal-based metrics. Its key tenet is to “do not use journal-based metrics, such as Journal Impact Factors (JIFs), as surrogate measures of the quality of individual research articles, to assess an individual scientist’s contributions, or in hiring, promotion, or funding decisions”. \nDORA also supports the idea that research should be assessed on its own merits and not influenced by the reputation of the journal in which it has been published. \nThe University of Southampton is a signatory of DORA.\nCoARA (Coalition of Advancing Research Assessment) \nThe CoARA initiative was launched in January 2022 and builds on DORA and other initiatives.  Its core principle is “that in the assessment of research, researchers and research organisations needs to recognise that diverse outputs, practices and activities contribute to the quality and impact of research.  This requires basing assessment primarily on qualitative judgement for which peer review is central, supported by the responsible use of quantitative indicators” (metrics). \nBarcelona Declaration on Open Research Information \nLaunched in April 2024, the Barcelona Declaration has the backing of research and funding organisations who have endorsed its commitment to “make openness the default for the research information we use and produce”. \nOne of its central recommendations is that we should move away from the use of closed and commercial data sources and work towards services and systems that support and enable open research information.  These open tools should then give access to metrics that have greater transparency and reproducibility. \n\n\n\nMetrics are used in a wide range of activities such as;\n\nResearch Assessment\nGrant applications\nRecruitment\nPromotion\nLeague Tables such as QS\n\nThank you for taking the time to complete this section of the course.\n\n\n\nComplete further sections\n\nSection 2 - Using metrics in research assessment\nSection 3 - Using metrics in personal applications and evaluations\nSection 4 - Assessing people using metrics\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 1"
    ]
  },
  {
    "objectID": "Section 1.html#section-1---responsible-research-metrics-overview",
    "href": "Section 1.html#section-1---responsible-research-metrics-overview",
    "title": "Section 1",
    "section": "",
    "text": "In this first section you will explore:\n\nWhat metrics are in the context of research\nThe principles of responsible metrics\nGuidelines around the responsible use of metrics\n\n\n\nMetrics are a quantitative snapshot of how research outputs perform. A research output is anything that disseminates research; for instance a journal article, a book, an exhibition or software. Metrics can look at one research output, at the outputs from a single researcher or at the entire output of a department or University. The majority of metrics primarily focus on journal articles but metrics may be available for other research types.\nThere are many different types of metrics and they each have a prescribed purpose.\nWhen using metrics it is important to have a question you are trying to answer so you can decide which metrics to use.\nUsing the wrong metric can have real world implications, from making false claims to causing reputational harm, which could impact future career ambitions or cause institutional damage.\n\n\n\nAs research disciplines and outputs are so varied there is no set step-by-step guide to carrying out a metrics analysis. Instead, there are a set of principles that should be followed: Transparency, Appropriateness, Equality, Reproducible and Continual Reassessment.\n\n\nTransparency in metrics means having an explanation in clear, simple language to ensure end-users understand the data used, its reliability and its limitations. \nEssentially what, where and when; \n\nWhat metrics were used? \nWhere did the data come from (sources)? \nWhen was the assessment made? – the assessment is only a snapshot in time as the data sources update over time \n\n\n\n\nThe use of Metrics should be fair.  You should only compare like-for-like i.e. make comparisons within a single discipline, a limited time period, or using a normalised metric.\nConsistency is important. If you can’t apply the same method across your analysis you should not use that method. \nBe aware that individuals may not be directly comparable due to length of service or time out of work such as for maternity leave.\n\nE.g. you should not compare an early career researcher with a researcher who has 20 years of experience unless you have restricted the comparison to a suitable time frame. \n\n\n\n\nAll metrics should be tailored to the focus of the analysis. Use metrics for their intended purpose only.\n\nFor example, you must not use a journal metric to infer the quality of an individual output or a person’s contribution to research.\n\nMetrics should only be used when necessary and should be used in conjunction with expert testimony rather than in isolation. \nWhen assessing a person, metrics must not be used as the sole source of information. This is especially true for employment status, but also for personal reputation in a formal or informal context. \n\nFor example, a highly cited paper might be highly cited because everyone disagrees with it\n\nIt is recommended that you use more than one metric to verify results. \n\n\n\nAnyone should be able to reproduce your results by using the explanation you have provided.\n\n\n\nContinually assess commonly used metrics, especially concerning appropriateness and equality. If a metric is no longer fit for purpose, it should not be used. \n\n\n\n\nA number of guidelines have been published which look to shape our approach to responsible metrics.  The University has its own responsible metrics policy which is based on the principles of DORA. Below are summaries of three key international initiatives, each of which advise on a different aspect: \nDORA (the Declaration of Research Assessment) \nDORA is a set of principles that were published in 2012.  They are designed to ensure that the quality and impact of scientific outputs is “measured accurately and evaluated wisely.”\nDORA focuses particularly on the use of journal-based metrics. Its key tenet is to “do not use journal-based metrics, such as Journal Impact Factors (JIFs), as surrogate measures of the quality of individual research articles, to assess an individual scientist’s contributions, or in hiring, promotion, or funding decisions”. \nDORA also supports the idea that research should be assessed on its own merits and not influenced by the reputation of the journal in which it has been published. \nThe University of Southampton is a signatory of DORA.\nCoARA (Coalition of Advancing Research Assessment) \nThe CoARA initiative was launched in January 2022 and builds on DORA and other initiatives.  Its core principle is “that in the assessment of research, researchers and research organisations needs to recognise that diverse outputs, practices and activities contribute to the quality and impact of research.  This requires basing assessment primarily on qualitative judgement for which peer review is central, supported by the responsible use of quantitative indicators” (metrics). \nBarcelona Declaration on Open Research Information \nLaunched in April 2024, the Barcelona Declaration has the backing of research and funding organisations who have endorsed its commitment to “make openness the default for the research information we use and produce”. \nOne of its central recommendations is that we should move away from the use of closed and commercial data sources and work towards services and systems that support and enable open research information.  These open tools should then give access to metrics that have greater transparency and reproducibility. \n\n\n\nMetrics are used in a wide range of activities such as;\n\nResearch Assessment\nGrant applications\nRecruitment\nPromotion\nLeague Tables such as QS\n\nThank you for taking the time to complete this section of the course.\n\n\n\nComplete further sections\n\nSection 2 - Using metrics in research assessment\nSection 3 - Using metrics in personal applications and evaluations\nSection 4 - Assessing people using metrics\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 1"
    ]
  },
  {
    "objectID": "Section 2.html",
    "href": "Section 2.html",
    "title": "Section 2",
    "section": "",
    "text": "This section is designed for:\n\nAnyone who is interested in how metrics contribute to research assessment\nAnyone who is involved in assessing research where metrics may be used as an assessment tool\n\nIn this this section you will explore:\n\nGuidance on how to use metrics in research assessment\nAn example question\n\n\n\nWhen you are assessing research using metrics you must have a question that you are trying to answer or task that you are trying to resolve. This question will help guide you to the correct metrics to use.\nWhen you have your question or task, you should break down into the following questions;\n\nWhat do I need to find out?​\nHow will I use the data?​\nHow do I need to present the results?​\nDo I need to use a specific source?​\nWhen is it required by? \nWhat do I need to find out?​\nWhat are the limitations of my data?\n\nWhat do I need to find out?​\nFirst of all you need to think carefully about what it is you’re trying to assess.\nOnce you have established this, the next stage is to identify which type of metrics and sources should be used to carry out the assessment.\nHow will I use the data?​\nAre you carrying out the analysis or are you providing data for someone else to analyse? Are you providing a list?\nYou should keep a record of what you do to the data and how you obtained it, so that anyone reading your results can replicate what you have done.\nHow do I need to present the results?​\nHow you present results can inform what source you use. Sometimes a table of results will be sufficient. However, there are also metric tools that can create visualisations for you, such as VOSviewer.\nWhatever format you decide on it should be appropriate to the question you are trying to answer.\nDo I need to use a specific source?​\nYou may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as equal as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.\nWhen is it required by? ​\nThe online sources you will use to find your initial data continuously update. It therefore might be appropriate to wait until nearer your deadline as further citations may have accumulated.\nIt’s important to be clear and transparent about the sources you use so always say when and where you got your data from.\nWhat are the limitations of my data?\nAll data has limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been.\nWhat, if any, assumptions have you had to make about your data to be able to proceed with your analysis?\nBe clear about the limitations of your data when you record how you’ve used it.\n\n\n\nSo how might we go about responsibly answering the question in papers in the same discipline or by the same author? Let us take this question.\nWhy did paper x do so much better than paper y?​\nTo begin with we need to decide how we are quantifying ‘better’.\nOnce this definition is decided you can source metrics and information and then do a like-for-like comparison.\nYou will need to be conscious of:\n\ndate i.e. is one paper newer?\nopen access status – can one paper be accessed by a wider potential audience?\njournal – if you have still not been able to determine from the information you have you may want to look at the publication. For example it may be that one paper was published in a society journal that only a few people read while the other was published in a big multidisciplinary journal.\n\nLet’s work it through with some hypothetical data.\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 25\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count:10\n\nFirst - what do we mean by better?\nbetter = cited more\nThis is a very simple assessment and looking at the data we can determine that Sample paper titled X has performed better, this will be because the paper is published in 2021, 2 years before Sample paper titled Y so citations have had more time to accumulate.\nHowever\nbetter=more impact\nIf we are looking at impact we can’t tell the impact of a paper from citation count alone so we can look for more data. The original data doesn’t tell us where the citation count comes from.\nAs I don’t know the source I will go to a database like Scopus and look at each paper to find more data. There I find the citation count, the Field Weighted Citation Impact and Plum X metrics.\n\nField Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average.\nPlum X metrics are an example of an almetrics these look at social media engagement, such as likes and shares, and engagement in platforms like Mendeley.\n\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 26\nField Weighted Citation Impact: 1.57\nPlum X - Captures: 10\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count: 12\nField Weighted Citation Impact: 1.52\nPlum X - Captures: 9\n\nThis tells me that although Sample paper titled X has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so actually both papers have about the same amount of impact so Sample paper titled X isn’t doing much better than Sample paper titled Y at all.\nAs you can see from this example, citation numbers don’t always give you the full picture. You need to consider other factors, such as age, discipline and journal reach, too. The question you ask will determine the answer you get. So take your time and don’t make snap decisions.\n\n\n\nComplete further sections\n\nSection 3 - Using metrics in personal applications and evaluations\nSection 4 - Assessing people using metrics\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 2"
    ]
  },
  {
    "objectID": "Section 2.html#section-2-assessing-research-using-metrics",
    "href": "Section 2.html#section-2-assessing-research-using-metrics",
    "title": "Section 2",
    "section": "",
    "text": "This section is designed for:\n\nAnyone who is interested in how metrics contributes to research assessment\nAnyone who is involved in assessing research where metrics may be used as an assessment tool\n\nIn this this section you will explore:\n\nGuidance on how to use metrics in research assessment\n\n\n\nWhen you are assessing research using metrics you must have a question that you are trying to answer or task that you are trying to resolve. This question will help guide you to the correct metrics to use.\nWhen you have this you should break down the question/task into the following questions.\n\nWhat do I need to find out?​\nHow will I use the data?​\nHow do I need to present the results?​\nDo I need to use a specific source?​\nWhen is it required by? \nWhat do I need to find out?​\nWhat are the limitations of my data?\n\nWhat do I need to find out?​\nFirst of all you need to think carefully about what it is you’re trying to assess.\nOnce you have established this, the next stage is to identify which type of metrics and sources should be used to carry out the assessment.\nHow will I use the data?​\nAre you carrying out the analysis or are you providing data for someone else to analyse? Are you providing a list?\nYou should keep a record of what you do to the data and how you obtained it, so that anyone reading can replicate what you have done.\nHow do I need to present the results?​\nSimilar to the question above, how you present results can inform what source you use. Some sources such as SciVal and Dimensions have the ability to create visualisations that can be inserted into a qualitative analysis. Sometimes a table of results will be sufficient. There are also standalone programs such as  VOSviewer that can create visualisations for you.\nWhatever format you decide on it should be appropriate to the question you are trying to answer.\nDo I need to use a specific source?​\nYou may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as equal as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.\nWhen is it required by? ​\nThe online sources you will use to find your initial data continuously update. It therefore might be appropriate to wait until nearer your deadline as further citations may have accumulated.\nIt’s important to be clear and transparent about the sources you use so always say when and where you got your data from.\nWhat are the limitations of my data?\nAll data has limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been.\nWhat, if any, assumptions have you had to make about your data to be able to proceed with your analysis?\nBe clear about the limitations of your data when you record how you’ve used it.\n\n\n\nSo how might we go about responsibly answering the question in papers in the same discipline or by the same author\nWhy did paper x do so much better than paper y?​\nTo begin with we need to decide how we are quantifying ‘better’\nOnce this definition is decided you can source these metrics and information and then do a like for like comparison on the metrics.\nYou will need to be conscious of\n\ndate i.e. is one paper newer?\nopen access status – can one paper be accessed by a wider potential audience?\njournal – if you have still not been able to determine from the information you have you may want to look at the publication. For example it may be that one paper was published in a society journal that only a few people read while the other was published in a big multidisciplinary journal.\n\nLet’s work it through with some hypothetical data.\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count 25,\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count 10\n\nFirst - what do we mean by better?\nbetter = cited more\nThis is a very simple assessment and looking at the data we can determine that Sample paper titled X has performed better, this will be because the paper is published in 2021, 2 years before Sample paper titled Y so citations have had more time to accumulate.\nHowever\nbetter=more impact\nIf we are looking at impact we can’t tell a papers impact from citation alone so we can look for more data. The original data doesn’t tell us where the citation count comes from.\nAs I don’t know the source I will go to database like Scopus and look at each paper to find more data. There I find out the citation count, the Field Weighted Citation Impact and Plum X metrics.\n\nField Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average,\nPlum X metrics are almetrics which looks at social media engagement such as likes and shares\n\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count 26\nField Weighted Citation Impact – 1.57\nPlum X - Captures 10\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count 12\nField Weighted Citation Impact – 1.52\nPlum X - Captures 9\n\nThis tells me that although Sample paper titled X has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so actually both papers have about the same amount of impact so Sample paper titled X isn’t doing much better than Sample paper titled Y at all.\nAs you can see from this example what question you ask will determine the answer you get. So take your time and don’t make snap decisions.\n\n\n\nComplete further sections\n\nSection 3a - Metrics in Recruitment and Promotion – As an applicant\nSection 3b - Metrics in Recruitment and Promotion – As an Assessor\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 2"
    ]
  },
  {
    "objectID": "Section 3a.html",
    "href": "Section 3a.html",
    "title": "Section 3",
    "section": "",
    "text": "This section is designed for: \n\nAnyone who is interested in understanding author metrics  \nAnyone who is applying for job roles, promotion cycles or applying for funding/grants applications where research may be used as an assessment tool \n\nIn this this section you will explore: \n\nWhat metrics are in the context of individual research \nSources of metrics \nLimitation of metrics \nGuidance on how to use metrics \n\n\n\nMetrics are a quantitative snapshot of how some of your research outputs have performed.  \nWhile your research output is a significant contribution, it’s only a part of your broader role as a researcher. Your knowledge of your contribution to your research area is greater than a snapshot list of numbers.  \nIf you haven’t been asked to provide metrics and if you can evidence the impact/your contribution without using metrics you don’t have to use them. Metrics do not supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution as a researcher. The only time you have to use metrics is if the assessment panel asks you to provide them. \n\n\n\nYou can obtain metrics by using an indexing database such as\n\nScopus  \nWeb of Science \nDimensions \nGoogle Scholar \nOpen Alex \n\nSome of these databases are free to use with an account and some of these are accessible via subscription. They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review your research that these systems have indexed. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed.  \n\nTypically journal articles and some books may be indexed. \nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences \nSmaller publishers may not index their journals \nIt can take six months and sometimes longer from publication for your article to be indexed \nContent may have been wrongly affiliated to you or your content wrongly affiliated to someone else.  \n\nCitations take a long time to accumulate \nThere is no way of knowing if citations are positive or negative \n\n\n\n\n\nYou could use metrics to evidence statements you make about your research. \nYou may be asked to provide a list such as your ‘top’ or ‘best’ papers.\n\nIf you do use metrics you need to use them responsibly, for example using the correct type of metric for your purpose \n\n\n\nExample of bad use \nI publish in top journals \nWhy this is bad – there is no context. It is vague. It is equating the quality of your research with the quality of the journal \nExample of good use \n[citation of paper] is my top performing paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better – there is a source identified and there are actual numbers, there is an idea of age of the paper from the citation, and you’ve demonstrated real world impact.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric which is gives a fairer assessment rather than a binary count. You could also talk about your impact more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date] and has a field weighted citation index of 4.3 (1 being average). It has been cited by [policy document] and following its publication I have been asked by several news agencies to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement.  \nWhy this is good – You’ve given a source and date for your metric, you’ve used a normalised metric which makes for a fairer comparison. You’ve related your article to real world impact and engagement \n\n\n\nA common question we see is authors asking for help creating a list of their ‘best’ research publications. How you quantify best will depend on the context of what you are applying for, your discipline, and what the assessor has asked for.\nSome will argue that ‘best’ means the highest cited, but that automatically puts your newest papers at a disadvantage, as citations take time to accumulate. Some will say ‘best’ is most impactful which can be harder to prove. If it hasn’t been defined for your by the people making the assessment, It is important to state how you have defined best and then use relevant evidence. So how could you evidence this? \n\nList of articles and their citation count – this favours your older papers \nNormalised metrics – this gives your newer content a fairer chance against older content \nAltmetrics – metrics about alternative types of research such as hits on a public facing website or about the social media engagement with an article, particularly useful when looking at impact \nMedia engagement – has your article been citated outside of academia such as the news \nPolicy citations – has your research been used in government policy \nPatent citations – has your research been used by someone filing a patent \n\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nComplete further sections\n\nSection 3b - Metrics in Recruitment and Promotion – As an Assessor\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  },
  {
    "objectID": "Section 3a.html#section-3a---metrics-in-promotion-and-recruitment--applicant",
    "href": "Section 3a.html#section-3a---metrics-in-promotion-and-recruitment--applicant",
    "title": "Section 3a",
    "section": "",
    "text": "This section is designed for: \n\nAnyone who is interested in understanding author metrics  \nAnyone who is applying for job roles, promotion cycles or applying for funding/grants applications where research may be used as an assessment tool \n\nIn this this section you will explore: \n\nWhat metrics are in the context of individual research \nSources of metrics \nLimitation of metrics \nGuidance on how to use metrics \n\n\n\nMetrics are a quantitative snapshot of how some of your research outputs have performed.  \nWhile your research output is a significant contribution, it’s only a part of your broader role as a researcher. Your knowledge of your contribution to your research area is greater than a snapshot list of numbers.  \nIf you haven’t been asked to provide metrics and if you can evidence the impact/your contribution without using metrics you don’t have to use them. Metrics do not supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution as a researcher. The only time you have to use metrics is if the assessment panel asks you to provide them. \n\n\n\nYou can obtain metrics by using an indexing database such as\n\nScopus  \nWeb of Science \nDimensions \nGoogle Scholar \nOpen Alex \n\nSome of these databases are free to use with an account and some of these are accessible via subscription. They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review your research that these systems have indexed. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed.  \n\nTypically journal articles and some books may be indexed. \nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences \nSmaller publishers may not index their journals \nIt can take six months and sometimes longer from publication for your article to be indexed \nContent may have been wrongly affiliated to you or your content wrongly affiliated to someone else.  \n\nCitations take a long time to accumulate \nThere is no way of knowing if citations are positive or negative \n\n\n\n\n\nYou could use metrics to evidence statements you make about your research. \nYou may be asked to provide a list such as your ‘top’ or ‘best’ papers.\n\nIf you do use metrics you need to use them responsibly, for example using the correct type of metric for your purpose \n\n\n\nExample of bad use \nI publish in top journals \nWhy this is bad – there is no context. It is vague. It is equating the quality of your research with the quality of the journal \nExample of good use \n[citation of paper] is my top performing paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better – there is a source identified and there are actual numbers, there is an idea of age of the paper from the citation, and you’ve demonstrated real world impact.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric which is gives a fairer assessment rather than a binary count. You could also talk about your impact more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date] and has a field weighted citation index of 4.3 (1 being average). It has been cited by [policy document] and following its publication I have been asked by several news agencies to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement.  \nWhy this is good – You’ve given a source and date for your metric, you’ve used a normalised metric which makes for a fairer comparison. You’ve related your article to real world impact and engagement \n\n\n\nA common question we see is authors asking for help creating a list of their ‘best’ research publications. How you quantify best will depend on the context of what you are applying for, your discipline, and what the assessor has asked for.\nSome will argue that ‘best’ means the highest cited, but that automatically puts your newest papers at a disadvantage, as citations take time to accumulate. Some will say ‘best’ is most impactful which can be harder to prove. If it hasn’t been defined for your by the people making the assessment, It is important to state how you have defined best and then use relevant evidence. So how could you evidence this? \n\nList of articles and their citation count – this favours your older papers \nNormalised metrics – this gives your newer content a fairer chance against older content \nAltmetrics – metrics about alternative types of research such as hits on a public facing website or about the social media engagement with an article, particularly useful when looking at impact \nMedia engagement – has your article been citated outside of academia such as the news \nPolicy citations – has your research been used in government policy \nPatent citations – has your research been used by someone filing a patent \n\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nComplete further sections\n\nSection 3b - Metrics in Recruitment and Promotion – As an Assessor\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3a"
    ]
  },
  {
    "objectID": "about.html#about",
    "href": "about.html#about",
    "title": "About",
    "section": "",
    "text": "This is version 1 of the Responsible Research Metrics Self Study module. The module was written by the Bibliometrics Team at the University of Southampton Library in 2024 and released in xxx 2025.\nThe module team is;\n\nKate Lapage\nClare Hemmings\nLorrayne Smith\n\nThe module’s foundations are taken from the university’s Responsible Research Policy. For help please contact the team via eprints@soton.ac.uk\nThis module is shared under a CC BY license so that it may be adapted to suit other institutions’ policies. We ask that this page remains with further information by the adapting organisation added beneath.",
    "crumbs": [
      "Home",
      "about"
    ]
  },
  {
    "objectID": "index.html#welcome-to-this-responsible-research-metrics-self-study-module",
    "href": "index.html#welcome-to-this-responsible-research-metrics-self-study-module",
    "title": "Responsible Research Metrics Self Study Module",
    "section": "",
    "text": "This self-study module can be completed in your own time. It covers the core principles of Responsible Research Metrics.\nThis module is designed for:\n\nAnyone who is interested in how research is measured and assessed\nAnyone involved in recruitment, reward and recognition processes, grant applications where research may be used as an assessment tool\n\n Everyone should complete Section 1 & 2, Section 3a and 3b about recruitment are optional based on your own personal requirements.\nFor help please contact the Library metrics team via eprints@soton.ac.uk",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Section 3b.html",
    "href": "Section 3b.html",
    "title": "Section 3b",
    "section": "",
    "text": "This section is designed for:\n\nAnyone interested in how author metrics contribute to research assessment\nAnyone undertakes or supports recruitment activities, promotion cycles or funding/grants applications where author metrics may be used as an assessment tool\n\nIn this section you will explore:\n\nWhat metrics are in the context of individual research\nSources and limitations of metrics\nWays a metrics can be used Guidance on how to use metrics\n\n\n\nMetrics are a quantitative snapshot of how some of a researcher’s research outputs have performed. There are multiple sources of metrics and many different types of metrics for different types of analysis.\nUsing metrics when assessing research outputs is a valuable but limited measure of a researcher’s contributions. Metrics should never be used as the sole assessment criteria and a researcher should not be forced to limit their worth to numbers which will not sure the full reflection of their contribution. Their expert judgment and knowledge of their own research are invaluable and should not be overshadowed by quantitative measures. A narrative approach can more accurately reflect the full breadth and depth of a researcher’s impact.\n\n\n\nYou can obtain metrics by using an indexing database such as\n\nScopus  \nWeb of Science \nDimensions \nGoogle Scholar \nOpen Alex \n\nSome of these databases are free to use with an account and some of these are accessible via subscription. They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review your research that these systems have indexed. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \n\n\n\n\nNot all research outputs will be indexed.  \n\nTypically journal articles and some books may be indexed. \nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences \nSmaller publishers may not index their journals \nIt can take up to six months from publication for articles to be indexed \nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else.  \n\nCitations take a long time to accumulate favouring older papers over new.\nLack of context\n\nThere is no way of knowing if citations are positive or negative \nvolume of citations vary across disciplines, if you are subject expert you may have the contextual knowledge of what to expect but if not it can be hard to determine if it is good or not.\n\nRequesting ‘top 10’ lists may automatically put early career researchers at a disadvantage\nDifferent software/databases count things in different ways so it may lead to results you cannot compare.\nResearchers using pseudo metrics such as H-index, this is a proxy of quality and not a true reflection of contribution\nResearchers may use metrics incorrectly or use the wrong type of metric for the assessment they are making for example using a journal metric to infer quality of their article.\n\n\n\n\nIf you need metrics to help you in your decision-making process there are several things you can do to make it fairer for researchers and easier for you to compare the results. The key is consistency. The ideal scenario is a like-for-like comparison\n\nSet a time frame e.g. 5 years,\n\nrestricting to a time frame makes it easier to compare academics at different career stages or researchers who might have had a career break.\n\nDefine what you mean by ‘best’ or ‘top’ publication so researchers can focus their statements/evidence and ensure that you are comparing the same thing\nspecify a particular source to use or a specific metrics to use\n\nSome metrics are software/database specific and some are subscription based so access could be\n\nAsk the metrics service to provide metrics for the group of candidates so that everyone will be assessed in the same way at the same time. Please contact us at eprints@soton.ac.uk to discuss\n\n\n\n\nResearchers may use metrics to evidence the claims made in their statements or they may provide a list of their outputs and associated metrics. So how do you tell if what they have provided is ‘good’. below are some examples.\nExample of a bad statement \nI publish in top journals \nWhy is this bad? – there is no context. It is vague. It is equating the quality of their research with the quality of the journal \nExample of a good statement\n[citation of paper] is my top-performing paper which according to [source such as Scopus or Google Scholar] has been cited 15 times. It has been cited by [policy document]  \nWhy is this better? – there is a source identified and there are actual numbers, there is an idea of the age of the paper from the citation and they’ve demonstrated real-world impact.  \nThe statement could be made better by saying when they took their data from the source as citations accumulate over time. The number of citations an article has will vary by discipline, in some disciplines 15 citations is very high whereas for others it will be very low, if you are not a subject expert it might be difficult to know if this is an expected volume or high or low.\nExample of best practice\n[citation of paper] is my top-performing paper according to Scopus on [date] and has a field weighted citation index of 4.3 (1 being average). It has been cited by [policy document] and following its publication several news agencies have asked me to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement please see appendices for detail.  \nWhy this is good? – They’ve given a source and date for the metric, They’ve used a normalised metric which makes for a fairer comparison. They’ve related their article to real world impact and engagement and provided you with\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own assessment please contact the metrics service for support.\n\n\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3b"
    ]
  },
  {
    "objectID": "Section 3b.html#section-3b---metrics-in-promotion-and-recruitment--assessor",
    "href": "Section 3b.html#section-3b---metrics-in-promotion-and-recruitment--assessor",
    "title": "Section 3b",
    "section": "",
    "text": "This section is designed for:\n\nAnyone interested in how author metrics contribute to research assessment\nAnyone undertakes or supports recruitment activities, promotion cycles or funding/grants applications where author metrics may be used as an assessment tool\n\nIn this section you will explore:\n\nWhat metrics are in the context of individual research\nSources and limitations of metrics\nWays a metrics can be used Guidance on how to use metrics\n\n\n\nMetrics are a quantitative snapshot of how some of a researcher’s research outputs have performed. There are multiple sources of metrics and many different types of metrics for different types of analysis.\nUsing metrics when assessing research outputs is a valuable but limited measure of a researcher’s contributions. Metrics should never be used as the sole assessment criteria and a researcher should not be forced to limit their worth to numbers which will not sure the full reflection of their contribution. Their expert judgment and knowledge of their own research are invaluable and should not be overshadowed by quantitative measures. A narrative approach can more accurately reflect the full breadth and depth of a researcher’s impact.\n\n\n\nYou can obtain metrics by using an indexing database such as\n\nScopus  \nWeb of Science \nDimensions \nGoogle Scholar \nOpen Alex \n\nSome of these databases are free to use with an account and some of these are accessible via subscription. They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review your research that these systems have indexed. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \n\n\n\n\nNot all research outputs will be indexed.  \n\nTypically journal articles and some books may be indexed. \nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences \nSmaller publishers may not index their journals \nIt can take up to six months from publication for articles to be indexed \nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else.  \n\nCitations take a long time to accumulate favouring older papers over new.\nLack of context\n\nThere is no way of knowing if citations are positive or negative \nvolume of citations vary across disciplines, if you are subject expert you may have the contextual knowledge of what to expect but if not it can be hard to determine if it is good or not.\n\nRequesting ‘top 10’ lists may automatically put early career researchers at a disadvantage\nDifferent software/databases count things in different ways so it may lead to results you cannot compare.\nResearchers using pseudo metrics such as H-index, this is a proxy of quality and not a true reflection of contribution\nResearchers may use metrics incorrectly or use the wrong type of metric for the assessment they are making for example using a journal metric to infer quality of their article.\n\n\n\n\nIf you need metrics to help you in your decision-making process there are several things you can do to make it fairer for researchers and easier for you to compare the results. The key is consistency. The ideal scenario is a like-for-like comparison\n\nSet a time frame e.g. 5 years,\n\nrestricting to a time frame makes it easier to compare academics at different career stages or researchers who might have had a career break.\n\nDefine what you mean by ‘best’ or ‘top’ publication so researchers can focus their statements/evidence and ensure that you are comparing the same thing\nspecify a particular source to use or a specific metrics to use\n\nSome metrics are software/database specific and some are subscription based so access could be\n\nAsk the metrics service to provide metrics for the group of candidates so that everyone will be assessed in the same way at the same time. Please contact us at eprints@soton.ac.uk to discuss\n\n\n\n\nResearchers may use metrics to evidence the claims made in their statements or they may provide a list of their outputs and associated metrics. So how do you tell if what they have provided is ‘good’. below are some examples.\nExample of a bad statement \nI publish in top journals \nWhy is this bad? – there is no context. It is vague. It is equating the quality of their research with the quality of the journal \nExample of a good statement\n[citation of paper] is my top-performing paper which according to [source such as Scopus or Google Scholar] has been cited 15 times. It has been cited by [policy document]  \nWhy is this better? – there is a source identified and there are actual numbers, there is an idea of the age of the paper from the citation and they’ve demonstrated real-world impact.  \nThe statement could be made better by saying when they took their data from the source as citations accumulate over time. The number of citations an article has will vary by discipline, in some disciplines 15 citations is very high whereas for others it will be very low, if you are not a subject expert it might be difficult to know if this is an expected volume or high or low.\nExample of best practice\n[citation of paper] is my top-performing paper according to Scopus on [date] and has a field weighted citation index of 4.3 (1 being average). It has been cited by [policy document] and following its publication several news agencies have asked me to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement please see appendices for detail.  \nWhy this is good? – They’ve given a source and date for the metric, They’ve used a normalised metric which makes for a fairer comparison. They’ve related their article to real world impact and engagement and provided you with\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own assessment please contact the metrics service for support.\n\n\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3b"
    ]
  },
  {
    "objectID": "Section 3a.html#section-3---metrics-in-promotion-and-recruitment--applicant",
    "href": "Section 3a.html#section-3---metrics-in-promotion-and-recruitment--applicant",
    "title": "Section 3",
    "section": "",
    "text": "This section is designed for: \n\nAnyone who is interested in understanding author metrics  \nAnyone who is applying for job roles, promotion cycles or applying for funding/grants applications where research may be used as an assessment tool \n\nIn this this section you will explore: \n\nWhat metrics are in the context of individual research \nSources of metrics \nLimitation of metrics \nGuidance on how to use metrics \n\n\n\nMetrics are a quantitative snapshot of how some of your research outputs have performed.  \nWhile your research output is a significant contribution, it’s only a part of your broader role as a researcher. Your knowledge of your contribution to your research area is greater than a snapshot list of numbers.  \nIf you haven’t been asked to provide metrics and if you can evidence the impact/your contribution without using metrics you don’t have to use them. Metrics do not supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution as a researcher. The only time you have to use metrics is if the assessment panel asks you to provide them. \n\n\n\nYou can obtain metrics by using an indexing database such as\n\nScopus  \nWeb of Science \nDimensions \nGoogle Scholar \nOpen Alex \n\nSome of these databases are free to use with an account and some of these are accessible via subscription. They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review your research that these systems have indexed. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed.  \n\nTypically journal articles and some books may be indexed. \nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences \nSmaller publishers may not index their journals \nIt can take six months and sometimes longer from publication for your article to be indexed \nContent may have been wrongly affiliated to you or your content wrongly affiliated to someone else.  \n\nCitations take a long time to accumulate \nThere is no way of knowing if citations are positive or negative \n\n\n\n\n\nYou could use metrics to evidence statements you make about your research. \nYou may be asked to provide a list such as your ‘top’ or ‘best’ papers.\n\nIf you do use metrics you need to use them responsibly, for example using the correct type of metric for your purpose \n\n\n\nExample of bad use \nI publish in top journals \nWhy this is bad – there is no context. It is vague. It is equating the quality of your research with the quality of the journal \nExample of good use \n[citation of paper] is my top performing paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better – there is a source identified and there are actual numbers, there is an idea of age of the paper from the citation, and you’ve demonstrated real world impact.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric which is gives a fairer assessment rather than a binary count. You could also talk about your impact more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date] and has a field weighted citation index of 4.3 (1 being average). It has been cited by [policy document] and following its publication I have been asked by several news agencies to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement.  \nWhy this is good – You’ve given a source and date for your metric, you’ve used a normalised metric which makes for a fairer comparison. You’ve related your article to real world impact and engagement \n\n\n\nA common question we see is authors asking for help creating a list of their ‘best’ research publications. How you quantify best will depend on the context of what you are applying for, your discipline, and what the assessor has asked for.\nSome will argue that ‘best’ means the highest cited, but that automatically puts your newest papers at a disadvantage, as citations take time to accumulate. Some will say ‘best’ is most impactful which can be harder to prove. If it hasn’t been defined for your by the people making the assessment, It is important to state how you have defined best and then use relevant evidence. So how could you evidence this? \n\nList of articles and their citation count – this favours your older papers \nNormalised metrics – this gives your newer content a fairer chance against older content \nAltmetrics – metrics about alternative types of research such as hits on a public facing website or about the social media engagement with an article, particularly useful when looking at impact \nMedia engagement – has your article been citated outside of academia such as the news \nPolicy citations – has your research been used in government policy \nPatent citations – has your research been used by someone filing a patent \n\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nComplete further sections\n\nSection 3b - Metrics in Recruitment and Promotion – As an Assessor\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  },
  {
    "objectID": "Section 4.html",
    "href": "Section 4.html",
    "title": "Section 4",
    "section": "",
    "text": "This section is designed for:\n\nAnyone interested in how author metrics contribute to research assessment\nAnyone who undertakes or supports recruitment activities, promotion cycles or funding/grants applications where author metrics may be used as an assessment tool\n\nIn this section you will explore:\n\nHow to use metrics in your assessment\nLimitations of metrics\n\n\n\nMetrics are a quantitative snapshot of how some of a researcher’s research outputs have performed. There are multiple sources of metrics and many different types of metrics for different types of analysis.\nUsing metrics when assessing research outputs is a valuable but limited measure of a researcher’s contributions. Metrics should never be used as the sole assessment criteria, any evaluation of a researcher’s performance should not be solely based on a set of numbers that don’t fully reflect their contribution. Their expert judgment and knowledge of their own research are invaluable and should not be overshadowed by quantitative measures. A narrative approach can more accurately reflect the full breadth and depth of a researcher’s\n\n\n\nIf you need metrics to help you in your decision-making process there are several things you can do to make it fairer for researchers and easier for you to compare the results. The key is consistency. The ideal scenario is a like-for-like comparison.\n\nSet a time frame e.g. 5 years,\n\nrestricting to a time frame makes it easier to compare academics at different career stages or researchers who might have had a career break.\n\nDefine what you mean by ‘best’ or ‘top’ publication(s) so researchers can focus their statements/evidence and ensure that you are comparing the same thing\nSpecify a particular source to use or a specific metrics to use\n\nSome metrics are software/database specific, some of which are available via subscription only and therefore may not be available to all candidates\n\nAsk the metrics service to provide metrics for the group of candidates so that everyone will be assessed in the same way at the same time. Please contact us at eprints@soton.ac.uk to discuss\n\n\n\n\n\nNot all research outputs will be indexed;\n\nTypically journal articles and some books may be indexed\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences \nSmaller publishers may not index their journals \nIt can take up to six months from publication for articles to be indexed \nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else.  \n\nCitations take a long time to accumulate favouring older papers over new\nLack of context\n\nThere is no way of knowing if citations are positive or negative \nVolume of citations vary across disciplines, if you are subject expert you may have the contextual knowledge of what to expect but, if not it can be hard to determine whether it is good or not\n\nRequesting ‘top 10’ lists may automatically put early career researchers at a disadvantage\nDifferent software/databases count things in different ways so it may lead to results you cannot compare.\nResearchers may use flawed metrics such as h-index, as a proxy of quality and not a true reflection of contribution\nResearchers may use metrics incorrectly or use the wrong type of metric for the assessment they are making; for example using a journal metric to infer the quality onto their article\n\nThe key to using metrics in an assessment of people is to be consistent and fair. If you are not a specialist in a subject area you may not have the contextual knowledge to know if the citation volume given is high, low or expected. In some disciplines 15 citations is very high whereas in others it is low.\nResearchers are ultimately more than just their scholarly output so you should never base your decision solely on metrics. These should be used as part of a number of ways to assess the researcher as a whole before any decision is made.\nThank you for taking the time to complete this section. This is the last part of this self-study module.\n\n\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 4"
    ]
  },
  {
    "objectID": "Section 4.html#section-4---assessing-people-using-metrics",
    "href": "Section 4.html#section-4---assessing-people-using-metrics",
    "title": "Section 4",
    "section": "",
    "text": "This section is designed for:\n\nAnyone interested in how author metrics contribute to research assessment\nAnyone who undertakes or supports recruitment activities, promotion cycles or funding/grants applications where author metrics may be used as an assessment tool\n\nIn this section you will explore:\n\nHow to use metrics in your assessment\nLimitations of metrics\n\n\n\nMetrics are a quantitative snapshot of how some of a researcher’s research outputs have performed. There are multiple sources of metrics and many different types of metrics for different types of analysis.\nUsing metrics when assessing research outputs is a valuable but limited measure of a researcher’s contributions. Metrics should never be used as the sole assessment criteria, any evaluation of a researcher’s performance should not be solely based on a set of numbers that don’t fully reflect their contribution. Their expert judgment and knowledge of their own research are invaluable and should not be overshadowed by quantitative measures. A narrative approach can more accurately reflect the full breadth and depth of a researcher’s\n\n\n\nIf you need metrics to help you in your decision-making process there are several things you can do to make it fairer for researchers and easier for you to compare the results. The key is consistency. The ideal scenario is a like-for-like comparison.\n\nSet a time frame e.g. 5 years,\n\nrestricting to a time frame makes it easier to compare academics at different career stages or researchers who might have had a career break.\n\nDefine what you mean by ‘best’ or ‘top’ publication(s) so researchers can focus their statements/evidence and ensure that you are comparing the same thing\nSpecify a particular source to use or a specific metrics to use\n\nSome metrics are software/database specific, some of which are available via subscription only and therefore may not be available to all candidates\n\nAsk the metrics service to provide metrics for the group of candidates so that everyone will be assessed in the same way at the same time. Please contact us at eprints@soton.ac.uk to discuss\n\n\n\n\n\nNot all research outputs will be indexed;\n\nTypically journal articles and some books may be indexed\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences \nSmaller publishers may not index their journals \nIt can take up to six months from publication for articles to be indexed \nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else.  \n\nCitations take a long time to accumulate favouring older papers over new\nLack of context\n\nThere is no way of knowing if citations are positive or negative \nVolume of citations vary across disciplines, if you are subject expert you may have the contextual knowledge of what to expect but, if not it can be hard to determine whether it is good or not\n\nRequesting ‘top 10’ lists may automatically put early career researchers at a disadvantage\nDifferent software/databases count things in different ways so it may lead to results you cannot compare.\nResearchers may use flawed metrics such as h-index, as a proxy of quality and not a true reflection of contribution\nResearchers may use metrics incorrectly or use the wrong type of metric for the assessment they are making; for example using a journal metric to infer the quality onto their article\n\nThe key to using metrics in an assessment of people is to be consistent and fair. If you are not a specialist in a subject area you may not have the contextual knowledge to know if the citation volume given is high, low or expected. In some disciplines 15 citations is very high whereas in others it is low.\nResearchers are ultimately more than just their scholarly output so you should never base your decision solely on metrics. These should be used as part of a number of ways to assess the researcher as a whole before any decision is made.\nThank you for taking the time to complete this section. This is the last part of this self-study module.\n\n\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 4"
    ]
  },
  {
    "objectID": "Section 2.html#section-2---using-metrics-in-research-assessment",
    "href": "Section 2.html#section-2---using-metrics-in-research-assessment",
    "title": "Section 2",
    "section": "",
    "text": "This section is designed for:\n\nAnyone who is interested in how metrics contribute to research assessment\nAnyone who is involved in assessing research where metrics may be used as an assessment tool\n\nIn this this section you will explore:\n\nGuidance on how to use metrics in research assessment\nAn example question\n\n\n\nWhen you are assessing research using metrics you must have a question that you are trying to answer or task that you are trying to resolve. This question will help guide you to the correct metrics to use.\nWhen you have your question or task, you should break down into the following questions;\n\nWhat do I need to find out?​\nHow will I use the data?​\nHow do I need to present the results?​\nDo I need to use a specific source?​\nWhen is it required by? \nWhat do I need to find out?​\nWhat are the limitations of my data?\n\nWhat do I need to find out?​\nFirst of all you need to think carefully about what it is you’re trying to assess.\nOnce you have established this, the next stage is to identify which type of metrics and sources should be used to carry out the assessment.\nHow will I use the data?​\nAre you carrying out the analysis or are you providing data for someone else to analyse? Are you providing a list?\nYou should keep a record of what you do to the data and how you obtained it, so that anyone reading your results can replicate what you have done.\nHow do I need to present the results?​\nHow you present results can inform what source you use. Sometimes a table of results will be sufficient. However, there are also metric tools that can create visualisations for you, such as VOSviewer.\nWhatever format you decide on it should be appropriate to the question you are trying to answer.\nDo I need to use a specific source?​\nYou may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as equal as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.\nWhen is it required by? ​\nThe online sources you will use to find your initial data continuously update. It therefore might be appropriate to wait until nearer your deadline as further citations may have accumulated.\nIt’s important to be clear and transparent about the sources you use so always say when and where you got your data from.\nWhat are the limitations of my data?\nAll data has limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been.\nWhat, if any, assumptions have you had to make about your data to be able to proceed with your analysis?\nBe clear about the limitations of your data when you record how you’ve used it.\n\n\n\nSo how might we go about responsibly answering the question in papers in the same discipline or by the same author? Let us take this question.\nWhy did paper x do so much better than paper y?​\nTo begin with we need to decide how we are quantifying ‘better’.\nOnce this definition is decided you can source metrics and information and then do a like-for-like comparison.\nYou will need to be conscious of:\n\ndate i.e. is one paper newer?\nopen access status – can one paper be accessed by a wider potential audience?\njournal – if you have still not been able to determine from the information you have you may want to look at the publication. For example it may be that one paper was published in a society journal that only a few people read while the other was published in a big multidisciplinary journal.\n\nLet’s work it through with some hypothetical data.\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 25\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count:10\n\nFirst - what do we mean by better?\nbetter = cited more\nThis is a very simple assessment and looking at the data we can determine that Sample paper titled X has performed better, this will be because the paper is published in 2021, 2 years before Sample paper titled Y so citations have had more time to accumulate.\nHowever\nbetter=more impact\nIf we are looking at impact we can’t tell the impact of a paper from citation count alone so we can look for more data. The original data doesn’t tell us where the citation count comes from.\nAs I don’t know the source I will go to a database like Scopus and look at each paper to find more data. There I find the citation count, the Field Weighted Citation Impact and Plum X metrics.\n\nField Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average.\nPlum X metrics are an example of an almetrics these look at social media engagement, such as likes and shares, and engagement in platforms like Mendeley.\n\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 26\nField Weighted Citation Impact: 1.57\nPlum X - Captures: 10\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count: 12\nField Weighted Citation Impact: 1.52\nPlum X - Captures: 9\n\nThis tells me that although Sample paper titled X has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so actually both papers have about the same amount of impact so Sample paper titled X isn’t doing much better than Sample paper titled Y at all.\nAs you can see from this example, citation numbers don’t always give you the full picture. You need to consider other factors, such as age, discipline and journal reach, too. The question you ask will determine the answer you get. So take your time and don’t make snap decisions.\n\n\n\nComplete further sections\n\nSection 3 - Using metrics in personal applications and evaluations\nSection 4 - Assessing people using metrics\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 2"
    ]
  },
  {
    "objectID": "Section 3.html",
    "href": "Section 3.html",
    "title": "Section 3",
    "section": "",
    "text": "This section is designed for: \n\nAnyone who is interested in understanding author metrics  \nAnyone who is applying for job roles, promotion cycles or applying for funding/grant applications where research may be used as an assessment tool \n\nIn this section you will explore: \n\nmetrics in the context of individual research \nSources of metrics \nLimitation of metrics \nGuidance on how to use metrics \n\n\n\nMetrics are a quantitative snapshot of how some of your research outputs have performed.  \nWhile your research output is a significant contribution, it’s only a part of your broader role as a researcher. Your knowledge of your contribution to your research area is greater than a snapshot list of numbers.  \nIf you haven’t been asked to provide metrics, and if you can evidence the impact or your contribution without using metrics, you don’t have to use them. Metrics do not supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution as a researcher. The only time you have to use metrics is if the assessment panel asks you to provide them. \n\n\n\nYou can obtain metrics by using an indexing database such as;\n\nScopus + \nWeb of Science +\nDimensions*\nGoogle Scholar*\nOpen Alex*\n\nSome of these databases are free to use with an account(*) and some of these are accessible via University of Southampton subscriptions(+). They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review how your research has been indexed by these systems. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed\n\nTypically journal articles and some books may be indexed\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences \nSmaller publishers may not index their journals \nIt can take up to six months from publication for your article to be indexed (sometimes even longer!)\nContent may have been wrongly affiliated to you or your content wrongly affiliated to someone else\n\nCitations take a long time to accumulate \nThere is no way of knowing if citations are positive or negative \n\n\n\n\n\nYou could use metrics to evidence statements you make about your research\nYou may be asked to provide a list such as your ‘top’ or ‘best’ papers\n\nIf you use metrics you need to use them responsibly, for example, using the correct type of metric for your purpose. \n\n\n\nExample of bad use \nI publish in top journals \nWhy this is bad – there is no context. It is vague. It is equating the quality of your research with the quality of the journal. \nExample of good use \n[citation of paper] is my top performing paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better – there is a source identified and there are actual numbers, there is an idea of age of the paper from the citation, and you’ve demonstrated real world impact.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric which is gives a fairer assessment rather than a binary count. You could also talk about your impact more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date you checked] and has a field-weighted citation index of 4.3 (1 being average). It has been cited by [policy document]. Following its publication I have been asked by several news agencies to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement.  \nWhy this is good – You’ve given a source and date for your metric, you’ve used a normalised metric which makes for a fairer comparison. You’ve related your article to real world impact and engagement. \n\n\n\nA common question we see is authors asking for help creating a list of their ‘best’ research publications. How you quantify best will depend on the context of what you are applying for, your discipline, and what the assessor has asked for.\nSome will argue that ‘best’ means the highest cited, but that automatically puts your newest papers at a disadvantage, as citations take time to accumulate. Some will say ‘best’ is most impactful which can be harder to prove. If it hasn’t been defined for you by the people making the assessment, it is important to state how you have defined best and then use relevant evidence.\nSo how could you evidence your ‘best’ publications? \n\nList of articles and their citation count – this favours your older papers \nNormalised metrics such as field-weighted citation index– this gives your newer content a fairer chance against older content \nAltmetrics – metrics about alternative types of research such as hits on a public-facing website, or social media engagement with an article are particularly useful when looking at impact\nMedia engagement – has your article been citated outside of academia such as the news? \nPolicy citations – has your research been used in government policy? \nPatent citations – has your research been used by someone filing a patent? \n\nWhen using metrics to assess research or during an application you should approach your task with as much rigour and mindfulness of methodology as you would when you undertake your normal research practices. The answer you will get will depend on the questions you ask. It should be clear as to how you have come to your conclusions so that your analysis is easily reproducible.\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nComplete further sections\n\nSection 4 - Assessing people using metrics - Section 4 is an optional part of this course for people who will take part in the assessment of people such as those on recruitment panels\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  },
  {
    "objectID": "Section 3.html#section-3---using-metrics-in-personal-applications-and-evaluations",
    "href": "Section 3.html#section-3---using-metrics-in-personal-applications-and-evaluations",
    "title": "Section 3",
    "section": "",
    "text": "This section is designed for: \n\nAnyone who is interested in understanding author metrics  \nAnyone who is applying for job roles, promotion cycles or applying for funding/grant applications where research may be used as an assessment tool \n\nIn this section you will explore: \n\nmetrics in the context of individual research \nSources of metrics \nLimitation of metrics \nGuidance on how to use metrics \n\n\n\nMetrics are a quantitative snapshot of how some of your research outputs have performed.  \nWhile your research output is a significant contribution, it’s only a part of your broader role as a researcher. Your knowledge of your contribution to your research area is greater than a snapshot list of numbers.  \nIf you haven’t been asked to provide metrics, and if you can evidence the impact or your contribution without using metrics, you don’t have to use them. Metrics do not supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution as a researcher. The only time you have to use metrics is if the assessment panel asks you to provide them. \n\n\n\nYou can obtain metrics by using an indexing database such as;\n\nScopus + \nWeb of Science +\nDimensions*\nGoogle Scholar*\nOpen Alex*\n\nSome of these databases are free to use with an account(*) and some of these are accessible via University of Southampton subscriptions(+). They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review how your research has been indexed by these systems. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed\n\nTypically journal articles and some books may be indexed\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences \nSmaller publishers may not index their journals \nIt can take up to six months from publication for your article to be indexed (sometimes even longer!)\nContent may have been wrongly affiliated to you or your content wrongly affiliated to someone else\n\nCitations take a long time to accumulate \nThere is no way of knowing if citations are positive or negative \n\n\n\n\n\nYou could use metrics to evidence statements you make about your research\nYou may be asked to provide a list such as your ‘top’ or ‘best’ papers\n\nIf you use metrics you need to use them responsibly, for example, using the correct type of metric for your purpose. \n\n\n\nExample of bad use \nI publish in top journals \nWhy this is bad – there is no context. It is vague. It is equating the quality of your research with the quality of the journal. \nExample of good use \n[citation of paper] is my top performing paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better – there is a source identified and there are actual numbers, there is an idea of age of the paper from the citation, and you’ve demonstrated real world impact.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric which is gives a fairer assessment rather than a binary count. You could also talk about your impact more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date you checked] and has a field-weighted citation index of 4.3 (1 being average). It has been cited by [policy document]. Following its publication I have been asked by several news agencies to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement.  \nWhy this is good – You’ve given a source and date for your metric, you’ve used a normalised metric which makes for a fairer comparison. You’ve related your article to real world impact and engagement. \n\n\n\nA common question we see is authors asking for help creating a list of their ‘best’ research publications. How you quantify best will depend on the context of what you are applying for, your discipline, and what the assessor has asked for.\nSome will argue that ‘best’ means the highest cited, but that automatically puts your newest papers at a disadvantage, as citations take time to accumulate. Some will say ‘best’ is most impactful which can be harder to prove. If it hasn’t been defined for you by the people making the assessment, it is important to state how you have defined best and then use relevant evidence.\nSo how could you evidence your ‘best’ publications? \n\nList of articles and their citation count – this favours your older papers \nNormalised metrics such as field-weighted citation index– this gives your newer content a fairer chance against older content \nAltmetrics – metrics about alternative types of research such as hits on a public-facing website, or social media engagement with an article are particularly useful when looking at impact\nMedia engagement – has your article been citated outside of academia such as the news? \nPolicy citations – has your research been used in government policy? \nPatent citations – has your research been used by someone filing a patent? \n\nWhen using metrics to assess research or during an application you should approach your task with as much rigour and mindfulness of methodology as you would when you undertake your normal research practices. The answer you will get will depend on the questions you ask. It should be clear as to how you have come to your conclusions so that your analysis is easily reproducible.\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nComplete further sections\n\nSection 4 - Assessing people using metrics - Section 4 is an optional part of this course for people who will take part in the assessment of people such as those on recruitment panels\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  }
]